{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration, InstructBlipConfig\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "PREPROCESING BLIP ENCODER to multiple context moment-wise state-action tokens\n",
    "\n",
    "MODEL 9:\n",
    "    Behavioral cloning Renas  transformer camera-lidar\n",
    "    1. TEXT-Image camera or (camera+map concatenation) ENCODER using InstructBLIP (frozen) \n",
    "    2. TEXT-Image camera or (camera+map concatenation) DECODER using InstructBLIP (frozen) for text generation\n",
    "    3. Cross-attention middle tokens to cls driving token MID TRANSFORMER\n",
    "    4. (im_prompt)-(action) history-aware causal driving Transformer GPT\n",
    "    Loss: cross-attention metrics going to CrossEntropyLoss \n",
    "    Similarity metric: First half of cross-attention\n",
    "\n",
    "DATA:\n",
    "    1. Behavioral cloning correct demonstrations (state-action episodes) \n",
    "\n",
    "    State: (image) or (im-map concatenation) (reworked h5), prompt \n",
    "\n",
    "    Actions in ros: position(x,y) orientation quternions (z, w)\n",
    "    Actions for model are explored (im-prompt description) and set as tokens vocabulary\n",
    "\n",
    "    2. Actions annotations\n",
    "    (Im) or (Im-map), prompt\n",
    "'''\n",
    "\n",
    "DATASET = '/data/renas/pythonprogv2/phd_xiaor_project/TSA_dataset/real/2A724_may/tsa_combined.h5'\n",
    "DEVICE = 'cuda:0'\n",
    "PROMPT = 'Do you see green cone on the image? Answer only that question'\n",
    "\n",
    "def look_im(im_i, generated_text):\n",
    "    im_i_np = im_i.numpy()\n",
    "    #im_i_np = im_i.numpy().transpose(1, 2, 0)  # Convert to HWC format for displaying\n",
    "    plt.imshow(im_i_np.astype(np.uint8))\n",
    "    plt.title(generated_text)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "class Renas9(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(Renas9, self).__init__()\n",
    "        self.device = device\n",
    "        self.blip_config = InstructBlipConfig.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
    "        self.d_model = self.blip_config.text_config.d_model\n",
    "        \n",
    "        self.processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
    "        self.processor.image_processor.do_rescale = True\n",
    "        self.processor.image_processor.do_resize = True\n",
    "        self.processor.image_processor.do_normalize = False\n",
    "\n",
    "        self.blip_model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\", torch_dtype=torch.bfloat16)\n",
    "        for param in self.blip_model.parameters():\n",
    "            param.requires_grad = False \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(DEVICE)\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device_i = torch.device(f'cuda:{i}')\n",
    "            print(f'Cuda Device {i}: ', device_i, torch.cuda.get_device_name(i))\n",
    "    else:\n",
    "        print('No CUDA devices available')\n",
    "        device = torch.device('cpu')\n",
    "    print('Current device: ',device)\n",
    "\n",
    "    new_dataset_path = DATASET[:-3]+'_model9_prep.h5'\n",
    "    model = Renas9(DEVICE).to(DEVICE)\n",
    "\n",
    "    im = []\n",
    "    action = []\n",
    "\n",
    "    with h5py.File(DATASET, 'r') as hdf:\n",
    "        im_group = hdf['states']\n",
    "        action_group = hdf['actions']\n",
    "        num_episodes = len(im_group)\n",
    "        print('Dataset contains episodes: ', num_episodes)\n",
    "\n",
    "        preprocess_timer_start = time.time()\n",
    "        with h5py.File(new_dataset_path, 'w') as new_hdf:\n",
    "            new_hdf_im_group = new_hdf.create_group('states')\n",
    "            new_hdf_action_group = new_hdf.create_group('actions')\n",
    "            for i in range(num_episodes):\n",
    "                episode = 'data_'+str(i)\n",
    "                for im_num in range(im_group[episode].shape[0]):\n",
    "                    im_i = torch.from_numpy(im_group[episode][im_num]).float()\n",
    "                    inputs = model.processor(images=im_i, text= PROMPT,return_tensors=\"pt\")\n",
    "                    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "                    batch_size = inputs['input_ids'].size(0)\n",
    "                    if 'decoder_input_ids' not in inputs:\n",
    "                        inputs['decoder_input_ids'] = torch.LongTensor([model.blip_config.text_config.bos_token_id]).repeat(batch_size, 1).to(inputs['input_ids'].device)\n",
    "                    outputs = model.blip_model.forward(**inputs, return_dict=True)\n",
    "                    #print('Last hidden state: ', outputs.language_model_outputs.encoder_last_hidden_state.shape)\n",
    "                    outputs = model.blip_model.generate(\n",
    "                            **inputs,\n",
    "                            do_sample=True,\n",
    "                            num_beams=5,\n",
    "                            max_length=512,\n",
    "                            min_length=1,\n",
    "                            top_p=0.9,\n",
    "                            repetition_penalty=2.5,\n",
    "                            length_penalty=0.5,\n",
    "                            temperature=1,\n",
    "                    )\n",
    "                    generated_text = model.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "                    print('\\n'+generated_text)\n",
    "                    look_im(im_i, generated_text)\n",
    "                #im_i = im_i.numpy()\n",
    "                #new_hdf_im_group.create_dataset(episode, data=im_i, dtype = np.float32, compression = 'gzip')\n",
    "                #a = action_group[episode][:]\n",
    "                #new_hdf_action_group.create_dataset(episode, data=a, dtype = np.float32, compression = 'gzip')\n",
    "\n",
    "    \n",
    "    #print('preprocess full time: ',time.time()-preprocess_timer_start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
